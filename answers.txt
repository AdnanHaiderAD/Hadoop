----Question 1------

The Hadoop Streaming utility command that I have used  to produce lower-case versions of  text files is as follows:
 
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large.txt -output /user/s0840844/data/output -mapper mapper-lowercase.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper-lowercase.py -reducer cat -jobconf mapred.reduce.tasks=1 -jobconf mapred.map.tasks=10

The mapper used for this job is an executable python script which is defined as follows: 

#Mapper: mapper-lowercase.py 
#The input text file is split into lines and is fed to the stdin of the mapper process 

#! /usr/bin/python
import sys
# The script sequentially reads each line from the stdin and outputs a lowercase version of it to stdout.
for line in sys.stdin:
	line= line.lower()
	print line[:-1]


The use of multiple mappers takes advantage of Hadoops shared-nothing architecture and 	
As specified by the question, the output must be a lowercase version of the file, hence the number of reducers used for the above job have been set to one .
The reducer here is the cat function which can seen as an identity map of the map outputs. 


---Question 2----

To remove duplicate sentences from the lower-cased version of the large.txt file, I have written the following executable scripts for the mapper and reducer functions:
 The executable script used to perform the mapper's task :
#Mapper: mapper-lowercasededup.py


#Hadoop allows  the execution of user-specified initalization and termination code to run at the beginning and end of each map and reduce task. Hence before processing lines from stdin, each map job is assigned a set that stores single copies of each sentence. In other words, we are actually adding the functionality of a combiner in the executable mapper script. 

#! /usr/bin/python

import sys
uniqlines=set() #This is a set that stores unique lines
for line in sys.stdin:

	line=line.strip() 
	if line not in uniqlines: 
		uniqlines.add(line)
		print line
	else:
		continue


The executable script created to perform the reducer task is:
#Reducer: reducer-lowercasededup.py

#! /usr/bin/python

import sys
#The reducer reads lines from stdin sequentially. It stores a copy of the previous line that has it has read in its buffer to ensure that it outputs only unique sentences to stdout.
previousline=""
for line in sys.stdin:
	
	line=line.strip()
	if line!=previousline:
		previousline=line
		print line
	else:
		continue
		



The use of 'line.strip()' in both these scripts removes whitespace characters from the front and back end of each line which thus allows sentences to b compared based on words only. The output of the above MapReduce execution will sometimes differ to the  output produced of the shell 'sort -u file.txt' command that sorts and removes duplicate lines from text files . Lets consider the following example:

Let the text file be test.txt whose contents are :
"
  Hello this is me
Nope this is not me
hello this is me
Nope this is not me
Nope this is me
haha
haha yea
haha"

Now the  shell command ' sort -u test.txt ' outputs the following 'unique'  lines :
  Hello this is me
haha
haha yea
hello this is me
Nope this is me
Nope this is not me

This command utilizes whitespace characters at the beginning of the text to distinguish sentences even when their content is the same.


wheres the "cat test| python mapper-lowercasededup.py|sort|python reducer-lowercasededup.py " outputs only unique sentences based on content:
hello this is me
nope this is not me
nope this is me
haha
haha yea


The Hadoop Streaming utility command that I have used to run the job:

hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large_lower.txt -output /user/s0840844/data/output -mapper mapper-lowercasededup.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper-lowercasededup.py -reducer reducer-lowercasededup.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer-lowercasededup.py -jobconf mapred.reduce.tasks=1 -jobconf mapred.map.tasks=10 -jobconf stream.map.output.field.separator=

#The input data is split into shards. Each map task is associated with a single block of data. In situations where duplicate copies of a sentence are located at different blocks which are not processed by the same mapper, the detection and removal of these duplicates will fail at the map stage. To address this issue, I have added the following command in the jobconf option :"-jobconf stream.map.output.field.separator= " . Defining the field separator of map outputs to be the empty string results in entire sentences being considered as keys.This will guarantee that identical strings are grouped together during the 'Shuffle and Sort' phase before being fed to the reducer. The reducer reads lines from stdin sequentially and stores a copy of the previous line that has it has read in its buffer which it uses to output only unique sentences to stdout.
#The use of multiple mappers takes of advantage of hadoop's share-nothing architecture. As specified by the question, the output must be a lowercase deduplicated version of the file, hence here as well the number of reducers used for the above job have been set to one.	



----Question 3----
The task in this section involves implementing an exact version of the shell wc command. 

The contents of the text files used  for this assignment comprise of Web data.  Web data consists of both alphanumeric and non-alphanumeric characters such as '{,'\' etc. The wc shell command uses the  whitespace '\w' and newline '\n'  characters to segment lines into words and hence strings  like "/^(([^()[\]\\" are now considered as words. 
To address this issue, I have created two different versions of the executable  script that implements the map function.

In version 1, The script that I have written to perform the map task is defined below:
#Mapper:mapper_wc.py 

#! /usr/bin/python

import sys,re

for line in sys.stdin:
	#removing non-alphanumeric characters
	#line= ' '.join(word for word in line.split() if word.isalnum())
	
	words= line.strip().split()# Removes whitespace characters at the beginning and end of the line and segment the line into a list of words using the whitespace character
	numberOfWords=len(words)
	print '%s\t%s' % (1,numberOfWords) # key =1(The number of current lines the mapper is reading which is always 1) and value= number of words in the line.



The output of each mapper is of the form "1 wordcount" where key=1(the number of lines the mapper is currently reading )  and value=wordcount 

Where as in version 2, the line " ' '.join(word for word in line.split() " has been uncommented. This  ensures  non-alphanumeric strings like '/^(([^()[\]\\' are removed from sentences before performing word counts because in english language we generally regard words as strings of alphanumeric characters that have semantic relevance.



The executable script created to perform the functions of the reducer is as follows:
#Reducer: reducer_wc.py
#! /usr/bin/python

import sys

lineCount=0
wordCount=0
for line in sys.stdin:
	linenum, wordnum =line.split('\t',1)# extracts key and value 
	lineCount= lineCount+int(linenum)
	wordCount= wordCount+int(wordnum)
print'%s\t%s' % (lineCount,wordCount)	


To compare the outputs produced by the two versions of Map Reduce program,  I have tested both versions on the text file 'small.txt' in order to compare their performance.  

Using version 1 of the mapper function, the unix commands used to run this job:
cat small.txt |python mapper_wc.py |sort |python reducer_wc.py 
10000	149552

Using version 2 of the mapper function, 
cat small.txt |python mapper_wc.py |sort |python reducer_wc.py 
10000	123763


Now in Unix, running the command  ' wc small.txt' outputs:
10000  149552 1014062 small.txt

#As the aim of this task is to emulate the exact behaviour of the 'wc' command, therefore for the remaining excercises I have used the first version of the mapper executable script.


On Hadoop, the following utility command can be used to run the above Map Reduce 
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/small.txt -output /user/s0840844/data/output -mapper mapper_wc.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper_wc.py -reducer reducer_wc.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc.py -jobconf mapred.reduce.tasks=1 -jobconf mapred.map.tasks=10 

Since the size of the file is small and to ensure that the results are stored in 1 file, the number of reducers is set to 1.

Using version 1 of the mapper function , the resultant map-reduce output is :
hadoop dfs -cat data/output/part-00000
10000  149552

Using version 2 of the mapper,function,the resultant map-reduce output is:
hadoop dfs -cat data/output/part-00000
10000	123763


---Question 4---
#Exact counting methods achieve 100% accuracy at the expense of of high memory usage. In situations where the emphasis on the performance level is greater than on the error rate, randomized counting methods can be implemented. Randomized counting methods aim to  allocate the smallest amount of space possible to the counter by down-sampling the events. For this task, probabilistic counting method is used to approximate counts. The central idea behind this methodology is that the exact count is approximated by the number b^f where b is fixed and f is exponent. The algorithm employs downsampling to increment the counter of f. Hence by storing exponents instead of actual counts, we therefore save a lot of memory space at the cost of accuracy.

#To make approximate counts for the number of words, I have written and implmented the following executable mapper and reducer scripts that uses probabilistic counting: 

#Mapper : mapper_wc_prob.py
#! /usr/bin/python

import sys

for line in sys.stdin:
	numberofwords= len(line.split())#counts the number of words in the current line
	
	for i in range(numberofwords):
		print 1 




Reducer used:

#! /usr/bin/python
import sys,random
counter=0
random.seed(random.randint(1,1000))
for line in sys.stdin:
	#count =1
	count=int(line.strip())
	if random.random()< pow(2,-counter):# if the random number generated < b^(-f) where b =2 then increment the  counter by 1
		counter+=1
		random.seed(random.randint(1,1000))# the random number generator is reinitialized to a different seed to ensure that the numbers generated resemble to numbers generated from a uniform distribution between 0 and 1
	else:
		continue
print counter
				

Similarly to make an approximate count on the number of lines: I have the 'cat' function as my mapper and the same reducer executable script as above with "count=int(line.strip())" commented out and "count=1" uncommented.				
			
Testing the probabilistic counting program on the small.txt file:

To get an approximate linecount, the hadoop utility command that I have used is as follows:

hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/small.txt -output /user/s0840844/data/output -mapper cat -reducer reducer_wc_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc_prob.py -jobconf mapred.reduce.tasks=1 

hadoop dfs -cat data/output/part-00000
13
Therefore the approximate line count is 2^13=8192


Similarly, to get an approximate wordcount, the corresponding streaming utility commnad used is:
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/small.txt -output /user/s0840844/data/output -mapper mapper_wc_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper_wc_prob.py -reducer reducer_wc_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc_prob.py -jobconf mapred.reduce.tasks=1 

hadoop dfs -cat data/output/part-00000
18

Therefore the approximate word count is 2^18=262144


----Question 5-----------

The hadoop utility command used to find the total number of sentences and words in the 'large.txt':
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large.txt -output /user/s0840844/data/output -mapper mapper_wc.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper_wc.py -reducer reducer_wc.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc.py -jobconf mapred.reduce.tasks=10 -jobconf mapred.map.tasks=10 

To take advantage of the shared-nothing architecture and to reduce the amount of computational load on a single node, multiple reducers have been used.To aggregate the counts I have used the same executable reducer script and ran the following hadoop utility command to run another MapReduce job on the output inorder to get the total tally :

hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input data/output/part-00000 -input data/output/part-00001 -input data/output/part-00002 -input data/output/part-00003 -input data/output/part-00004 -input data/output/part-00005 -input data/input5/part-00006 -input data/output/part-00007 -input data/output/part-00008 -input data/output/part-00009 -output data/output2 -mapper cat -reducer reducer_wc.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc.py -jobconf mapred.reduce.tasks=1 -jobconf mapred.map.tasks=10 

Similarly for the deduplicated version of the 'large.txt, the streaming utility commands used are as follows:
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large_lower_dedup.txt -output /user/s0840844/data/output -mapper mapper_wc.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper_wc.py -reducer reducer_wc.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc.py -jobconf mapred.reduce.tasks=10 -jobconf mapred.map.tasks=10 

To aggregate counts:
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input data/output/part-00000 -input data/output/part-00001 -input data/output/part-00002 -input data/output/part-00003 -input data/output/part-00004 -input data/output/part-00005 -input data/input5/part-00006 -input data/output/part-00007 -input data/output/part-00008 -input data/output/part-00009 -output data/output2 -mapper cat -reducer reducer_wc.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc.py -jobconf mapred.reduce.tasks=1 -jobconf mapred.map.tasks=10 

 Exact counting method
 text file		(line count)  (word count)
 large.txt                  5000000	69760386
 large_lower_dedup.txt      3148155	49369862

Observation:
The numbers observed satisfy the expected pattern. Due to the removal of duplicate sentences, the deduplicated version will have lesser number of sentences and words.
 
b) Using the Probabilistic counting method, the following commands have been executed to get approximate sentence and word counts:

The utility command used to get an approximate line count for the 'large.txt':
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large.txt -output /user/s0840844/data/output -mapper cat -reducer reducer_wc_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc_prob.py -jobconf mapred.reduce.tasks=1 

hadoop dfs -cat data/output/part-00000
21

Similarly, for the dedeuplicated version the command used is as follows:
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large_lower_dedup.txt -output /user/s0840844/data/output -mapper cat -reducer reducer_wc_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc_prob.py -jobconf mapred.reduce.tasks=1 

hadoop dfs -cat data/output/part-00000
21


Now,to estimate the wordcount of the 'large.txt' I have used the following command:
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large.txt -output /user/s0840844/data/output -mapper mapper_wc_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper_wc_prob.py -reducer reducer_wc_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc_prob.py -jobconf mapred.reduce.tasks=1 

hadoop dfs -cat data/output/part-00000
25

hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large_lower_dedup.txt -output /user/s0840844/data/output -mapper mapper_wc_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper_wc_prob.py -reducer reducer_wc_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc_prob.py -jobconf mapred.reduce.tasks=1

hadoop dfs -cat data/output/part-00000
25

 Probabilistic counting method
 text file		(line count)       (word count)
 large.txt                  2^21=2097152        2^25=   33554432
 large_lower_dedup.txt      2^21=2097152	2^25=   33554432

Observation:
Randomized counting methods aim to reduce the amount of memory space at the cost of accuracy. Hence it is expected that none of these numbers correspond to exact figures.

At first glance it might seem strange to an observer to see that the algorithm generates the same approximations for the line count and word count even though the deduplicated version contains less sentences and words. As I have already mentioned, the exact count is approximated by the number b^f (where b here is fixed and equal to 2 and f is exponent) and the algorithm employs downsampling to increment the counter of f. Thus on average, it takes 1 impulse for the counter to go from value 0 to value 1, then on average 2 more impulses to from 1 to 2, then on average 4 more impulses from 2 to 3, and so forth. By impulse here, I mean the number of lines the reducer has encountered so far. Therefore, in the case of counting words, to increment the counter of f to 26, the reducer on average must have seen 2^26= 67108864 words > the actual number of words in large.txt due to which the counter's increment has stopped after 25.

However, since this is a randomized method, it is also possible for the approximate counts to exceed the actual count on different runs. 
For example, when I ran the randomized version of the word counting algorithm again, I got the value of f to be 26 for the 'large.txt' and 25 for the deduplicated version.
Reason: Although the number 1/(2^f) decreases exponentially as f increases, in some runs, it is possible to generate random numbers which are less that 1/(2^f) even when f is large. 


----Question 6-----

# In section 6, the required task is to create executable mapper and reducer scripts to run a map-reduce job that finds all the  three word sequences and their exact(approximate) respective frequencies in the deduplicated version of the large.txt file. Both the large.txt and its deduplicated version consists of both alphanumeric and non-alphanumeric characters since their contents consist of web data. It is therefore neccessary to add additional code in the  mapper executable script to filter out the non-alphanumeric strings such as "/^(([^()[\]\\"  from the deduplicated version when extracting words to form three word sequences.  


#The executable script used to perform the mapper's function is as follows:
#Mapper:mapper_wdseq.py


#! /usr/bin/python

import sys

for line in sys.stdin:
	#removing non-alphanumeric strings
	line= ' '.join(word for word in line.split() if word.isalnum()) # ensures  non-alphanumeric strings like '/^(([^()[\]\\' are removed from sentences before performing three word sequence counts.
	wordlist= line.split()#using the white space character to segment the string into a list of words. This command also removes the newline character '\n' at the end of each line(Each line read from std.in contains a string of characters followed by the newline character).
	for index,word in enumerate(wordlist):
		if index < len(wordlist)-2:
			seq ='%s' % (' '.join([wordlist[index],wordlist[index+1],wordlist[index+2]]))# creating three-word sequences
			print '%s\t%s' % (seq,1)
		else:
			break


#The executable script used to perform the reducer's function is as follows:
#Reducer:reducer_wdseq.py

#! /usr/bin/python

import sys

prev_Seq=""
counter=0
for line in sys.stdin:
	
	seq,count =line.strip().split('\t',1);#extracting key and value
	count=int(count)
	if prev_Seq==seq:
		counter=counter+count
	else:
		if prev_Seq!="":
			print '%s\t%s' % (prev_Seq,counter)# print three word sequence and its corresponding total count
		prev_Seq=seq
		counter=count	
print '%s\t%s' % (prev_Seq,counter)

The reducer reads lines from stdin sequentially. It stores a copy of the previous line in its buffer and compares it with the current line. If both lines are identical then it updates the counter by 1. Else it prints the previous line with current counter value and before setting the counter to 1.

#The map-reduce job submitted to find the exact frequency counts of all three-word sequences:
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large_lower_dedup.txt -output /user/s0840844/data/output -mapper mapper_wdseq.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper_wdseq.py -reducer reducer_wdseq.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wdseq.py -jobconf mapred.reduce.tasks=10 -jobconf mapred.map.tasks=10

To reduce the computational load on a single node and to reduct the time taken to complete a job, multiple reducers have been specified in the userEnvironment side ofthe Hadoop job configuration.



Since there is a large number of possible three-word sequences, a considerable amount of memory is needed for the counters associated with each unique three word sequence. 
The following executable script is used as the reducer that down samples the counts associated with each sequence and hence reduces the amount of memory used to store the counts by only storing the exponents of b^(f) where b^(f) is an approximation of the true count. For this part of the excercise, I have set b=1.2( See Question 7 for an explanation)
#Reducer:reducer_wdseq_prob.py

#! /usr/bin/python
import sys, random
counter=0
prev_Seq=""
seq_counter=0
random.seed(random.randint(1,1000))#initializing the seed of the random number generator
for line in sys.stdin:
	seq, count =line.strip().split('\t',1);#extract key and value
	count=int(count)
	if prev_Seq==seq:
		if random.random()<pow(1.2,-counter):# if random number generated < b^(-f) where b =1.2 then increment the current three-word sequence 's count
			counter+=1
			seq_counter=seq_counter+count
		else:
			continue
	else:
		if prev_Seq!="":
			print '%s\t%s' % (prev_Seq,seq_counter)
		prev_Seq=seq
		counter=1
		seq_counter=count
		random.seed(random.randint(1,1000))# re-initializing the seed of the random number generator when we see a new sequence

		
			
print '%s\t%s' % (prev_Seq,seq_counter)	

As the reducer sequentially reads lines of stdin, for each new sequence the reducer comes accross, the seed of the random number generator is re-initialized. This ensures that the sequence of random numbers generated doesnt depend on a particular seed.		
	
#The map-reduce job submitted to find the approximate frequency counts of all three-word sequences:
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large_lower_dedup.txt -output /user/s0840844/data/output -mapper mapper_wdseq.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper_wdseq.py -reducer reducer_wdseq_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wdseq_prob.py -jobconf mapred.reduce.tasks=10 -jobconf mapred.map.tasks=10			




---Question 7---
# To take advantage of the shared-nothing architecture and to reduce the amount of computational load on a single node, I have used multiple reducers when running the map-reduce job to count the exact (and approximate )frequencies of all three word sequences that occur in a given text file. Utilizing  muliple reducers leads to the resultant output being partitioned according to the number of specified reduce tasks. Therefore to retrieve the top twenty three word sequences it is necessary to use  and combine data from all the output partitions and apply secondary sorting to sort the data according to frequency counts.



To rank the the three word sequences according to their exact frequencies, I have used the following command that employs secondary sorting:
The input of this MapReduce job  is  the output of the job used to tally the exact three-word sequence counts.

hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input data/output/part-00000 -input data/output/part-00001 -input data/output/part-00002 -input data/output/part-00003 -input data/output/part-00004 -input data/output/part-00005 -input data/output/part-00006 -input data/output/part-00007 -input data/output/part-00008 -input data/output/part-00009 -output data/output2 -mapper cat -reducer cat -jobconf stream.num.map.output.key.fields=2 -jobconf mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -jobconf mapred.text.key.comparator.options=-k2,2nr -jobconf mapred.reduce.tasks=1

# The command "stream.num.map.output.key.fields=2" creates composite intermediate keys of the form (sequence,count)
# The command "-jobconf mapred.text.key.comparator.options=-k2,2nr" performs secondary sorting   and hence sorts the outputs by the second field of the keys i.e the counts

The twenty most frequent sequences found by the exact method :
three-word sequences   count
one of the		16659	
as well as		11526	
a lot of		8698	
some of the		6827	
if you are		6743	
be able to		6045	
part of the		6032	
you want to		5740	
this is a		5473	
if you have		5459	
in order to		5403	
see all leads		5373	
the end of		5186	
to be a	        	5112	
there is a		4579	
the united states	4428	
of the most		4384	
out of the		4205	
it is a	        	4191	
a number of		4166	


Similarly, to rank the the three word sequences according to their approximate frequencies, I have used the same command but the input this time is the output of the  job used to get the approximate three-word sequence counts.




The twenty most frequent sequences found by the randomized method :
three-word sequences	Exponent f(b^f represents an approximate count)
as well as		44	
one of the		43	
it is a			41	
new meetup groups	40	
some of the		40	
the university of	40	
a lot of		40	
this is the		40	
if you have		39	
this is a		39	
the use of		39	
see all leads		39	
a number of		38	
the end of		38	
to be a			38	
you want to		38	
if you are		38	
of the most		38	
the fact that		38	
be able to		38


Setting the value of b to its default value 2 causes the estimates of the true frequencies to be inaccurate up to a binary order of magnitude. The aim here is to find the 20 most frequent three-word sequences. Hence inexact counts can be used as long as we get a relatively accurate ranking. To improve the accuracy of the ranking, I have used a smaller value for 'b'.  Using a smaller base allows for obtaining more accurate estimates at the cost of a smaller maximum frequency while using almost the same number of bits. The reducer now has to read lesser number of lines on average before incrementing the counter. Although having b^f < 2^f  results in the approximate count being smaller, the number of times the reducer updates the counter is now greater on average. And the since the value of the exponent f determines the ranking of the three word sequences, we are more likely to get a better accurate ranking than running the MapReduce job with the default value 2.

-----Question 8---
To take the advantage of local aggregation, the functionality of a combiner is incorporated directly inside the mapper.

#Mapper fused with combiner function: mapper-wdseq_combiner.py
#! /usr/bin/python

import sys
book =dict([])#data structure that tallies up counts of three-word sequences
for line in sys.stdin:
	#removing non-alphanumeric strings	
	line= ' '.join(word for word in line.split() if word.isalnum())
	wordlist= line.split()#using the white space chacracter to segment the string
	for index,word in enumerate(wordlist):
		if index < len(wordlist)-2:
			seq ='%s' % (' '.join([wordlist[index],wordlist[index+1],wordlist[index+2]]))# creating three-word sequences
			if book.has_key(seq):
				book[seq]= book[seq]+1 #if the mapper has already seen the sequence, increase the the tally of this sequence by 1
			else:
				book[seq]= 1
			
		else:
			break
	
		
for key,value in book.iteritems():	
	print '%s\t%s' % (key, value) 


The implementation of this version reduces the amount of intermediate data translate from the mappers to reducers and hence increases algorithmic efficiency.This new mapper executable script ensures that the number of key-value pairs  emitted by each mapper is restricted to the number of unique three word sequences in each block.

Due to the increased algorithmic effeciency, the time taken to complete a streaming map-reduce job is going to be smaller. This can be confirmed by the using the shell 'time' command to compare the time taken in completing the three word sequence counting job between using an 'in-mapper combiner' script  and  normal mapper executable script.

#Using in-mapper combiner script: 
time hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large_lower_dedup.txt -output /user/s0840844/data/output -mapper mapper-wdseq_combiner.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper-wdseq_combiner.py -reducer reducer_wdseq.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wdseq.py -jobconf mapred.reduce.tasks=10 -jobconf mapred.map.tasks=10

Time taken:
real	2m21.299s
user	0m4.032s
sys	0m0.404s

# Using the normal mapper script
time hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large_lower_dedup.txt -output /user/s0840844/data/output -mapper mapper_wdseq.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper_wdseq.py -reducer reducer_wdseq.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wdseq.py -jobconf mapred.reduce.tasks=10 -jobconf mapred.map.tasks=10


real	3m11.355s
user	0m4.080s
sys	0m0.412s


---Question 9---
To compare the amount of space used by the exact method to the size of the space used by randomized version, I have run the MapReduce jobs discussed in question 5 to find the exact and approximate counts for the number of sentences and words.
The text file used is 'large.txt'
The number of bytes  to store the exact sentence and word frequencies is 17
Command used to check the number of bytes:
hadoop dfs -cat data/output2/part-00000|wc -c
     17
[Note The output of MapReduce jobs that involves using exact counting methods are stored in the data/output2 directory]
Whereas using probabilistic method, the number bytes used to store approximate counts for the number of sentences and words is 8

Number of bytes used to store approximate word counts=4
[Note The output of MapReduce jobs that involves using approximate counting methods are stored in the data/output directory]
Command used:
hadoop dfs -cat data/output/part-00000|wc -c
4

Number of bytes used to store approximate sentence counts=4
Command used:
hadoop dfs -cat data/output/part-00000|wc -c
4

----Question 10--------


For the last task, I have run the following hadoop utility commands three times to observe the top 20 most frequent three-word sequences produced by employing a probabilistic counting method:

hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large_lower_dedup.txt -output /user/s0840844/data/output -mapper mapper_wdseq.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper_wdseq.py -reducer reducer_wdseq_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wdseq_prob.py -jobconf mapred.reduce.tasks=10 -jobconf mapred.map.tasks=10			

Using Secondary sorting to list the top 20 most frequent three-word sequences:
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input data/output/part-00000 -input data/output/part-00001 -input data/output/part-00002 -input data/output/part-00003 -input data/output/part-00004 -input data/output/part-00005 -input data/output/part-00006 -input data/output/part-00007 -input data/output/part-00008 -input data/output/part-00009 -output data/output2 -mapper cat -reducer cat -jobconf stream.num.map.output.key.fields=2 -jobconf mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator -jobconf mapred.text.key.comparator.options=-k2,2nr -jobconf mapred.reduce.tasks=1

Output produced by the first run:

three-word sequences	Exponent f(b^f represents an approximate count)[ b is set to its default value 2]
some of the		14	
a variety of		13	
the united states	13	
you have to		13	
this is a		13	
in new meetup		13	
is one of		13	
in the united		13	
it comes to		12	
part of the		12	
there are many		12	
this is not		12	
we will be		12	
there is a		12	
mont da klask		12	
it was a		12	
back in the		12	
the time of		12	
version of the		12	
in the same		12


Output produced by the second  run:
three-word sequences	Exponent f(b^f represents an approximate count)[ b is set to its default value 2]
some of the		14	
a variety of		13	
the united states	13	
you have to		13	
is one of		13	
in the united		13	
this is a		13	
in new meetup		13	
mont da klask		12	
it was a		12	
back in the		12	
the time of		12	
it comes to		12	
part of the		12	
there are many		12	
this is not		12	
we will be		12	
there is a		12	
as well as		12	
out of the		12


Output produced by the third run:
three-word sequences	Exponent f(b^f represents an approximate count)[ b is set to its default value 2]
some of the		14	
a variety of		13	
the united states	13	
you have to		13	
this is a		13	
is one of		13	
in the united		13	
in new meetup		13	
mont da klask		12	
it was a		12	
back in the		12	
the time of		12	
new york times		12	
in the and		12	
as well as		12	
out of the		12	
here are some		12	
there was a		12	
version of the		12	
in the same		12

Observation:
The approximate count of each three-word sequence is given by 2^f where f denotes the number of times the reducer downsamples observations corresponding to a particular sequence. The Second sorting  utility provided by Hadoop uses these exponents f to sort the three-word sequences. The estimates of the true frequencies  for each three word sequence is  inaccurate up to a binary order of magnitude. Compared to the list produced by the exact method, the top twenty most frequent three-word sequences found by the randomised version is fairly inaccurate in all 3 runs. On close inspection, it can be observed that there is a considerable variance associated with the rankings. Although the rankings of the top 6 three-word sequences is fairly consistent through out the  3 runs, the order in which the other sequences are ranked changes from run to run. For example: in the first run, the 19th and 20th most frequent three word sequences are " version of the" and "in the same", whereas in the 2nd run  these sequences are "as well as" and "out of the"  and finally in the 3rd run, the corresponding sequences are  " version of the" and "in the same".




Method Used to reduce variance:
The variance associated with the frequency estimate given by the probabilistic counting  method is (b-1)*n*(n+1)/2 where n is the true frequency . In the default case where b=2, the corresponding variance becomes n*(n+1)/2. Using  values of between 1 and 2 reduces the variance associated with the approximate estimate and allows for more accurate estimates at the cost of a smaller maximum frequency, using the same number of bits. Hence for our three-word sequence counting problem, the sampling rate for each three word sequence is now greater on average thus resulting in a more accurate ranking. 


	

By setting b=1.2 and running the twenty most frequent three word sequences, :
three-word sequeces	Exponent f(b^f represents an approximate count)
as well as		44	
one of the		43	
it is a			41	
new meetup groups	40	
some of the		40	
the university of	40	
a lot of		40	
this is the		40	
if you have		39	
this is a		39	
the use of		39	
see all leads		39	
a number of		38	
the end of		38	
to be a			38	
you want to		38	
if you are		38	
of the most		38	
the fact that		38	
be able to		38


Compared to the older version, the new version provides a more accurate ranking for the top 20 most frequent word sequences.



Limitation:

Although using a smaller base increases the frequency of updates for a counter, the maximum frequency given by the probabilistic counting method on average will now be less than before. To illustrate this fact, I have run the hadoop utility command (discussed in Question 4) multiple times to get estimates of the word count corresponding to the large file  using b=2 and b=1.2: 


Command Used:
hadoop jar /opt/hadoop/hadoop-0.20.2/contrib/streaming/hadoop-0.20.2-streaming.jar -input /user/s0840844/data/input/large.txt -output /user/s0840844/data/output -mapper mapper_wc_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/mapper_wc_prob.py -reducer reducer_wc_prob.py -file /afs/inf.ed.ac.uk/user/s08/s0840844/ExtremeComputing/hadoop/assignmentCode/reducer_wc_prob.py -jobconf mapred.reduce.tasks=1 

Using b=2, Text file :'large.txt'
Run  	Exponent f       Approximate word count 
1       25		2^25 =33554432
2	26		2^26 =67108864
3	26		2^26 = 67108864

The average approximate word count given the probabilistic method based on the above 3 runs: 55924053

Using b=2, Text file :'large.txt'
Run  	Exponent f       Approximate word count 
1       89		1.2^89=11146304
2	90		1.2^90=13375565
3	89		1.2^89=11146304

The average approximate word count given the probabilistic method based on the above 3 runs: 11889391

The actual word count for the text file is 69760377. Using a base that results in a greater maximum frequency is more ideal with respect to this problem. As it can seen from the above, the maximum count given by the probabilistic counting menthod is closer to the actual count on average when we set b=2 than setting b=1.2.



